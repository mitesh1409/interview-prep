# Scenario: You noticed a sudden increase in latency for your Node.js application hosted on AWS. How would you investigate and resolve this issue?

Below is a **structured, interview-ready approach** you can use to explain how you would **investigate and resolve a sudden latency increase** in a Node.js application running on AWS. This aligns with how senior engineers and SREs are expected to reason.

---

## 1. Immediate Triage (Stabilize & Scope)

**Objective:** Confirm the issue, assess blast radius, and prevent further impact.

### Actions

* **Confirm the symptom**

  * Check application response times (P95 / P99), error rates, and throughput.
  * Validate whether latency is:

    * Global or regional
    * Affecting all endpoints or specific APIs
* **Identify recent changes**

  * Code deployments
  * Infrastructure changes (Auto Scaling, Load Balancer config)
  * Dependency updates (DB, cache, third-party APIs)

### AWS Tools

* Amazon **CloudWatch dashboards**
* AWS **X-Ray service map**
* Load Balancer metrics (ALB / NLB)

---

## 2. Infrastructure-Level Investigation

**Objective:** Determine whether latency is caused by compute, networking, or scaling issues.

### EC2 / ECS / EKS

* Check:

  * **CPU utilization**
  * **Memory pressure**
  * **Disk I/O wait**
  * **Network latency**
* Look for:

  * CPU throttling
  * Garbage collection spikes
  * Container restarts (OOMKilled)

### Auto Scaling

* Verify:

  * Scaling events triggered too late
  * Insufficient instance count
  * Wrong instance types

### Load Balancer

* ALB metrics:

  * `TargetResponseTime`
  * `HTTPCode_Target_5XX_Count`
* Ensure:

  * Healthy targets
  * No uneven traffic distribution

---

## 3. Node.js Application Analysis

**Objective:** Identify bottlenecks inside the application runtime.

### Event Loop Health

* Monitor:

  * Event loop lag
  * Blocked synchronous operations
* Typical causes:

  * CPU-heavy loops
  * JSON serialization on large payloads
  * Synchronous filesystem calls

### Memory & GC

* Check:

  * Heap growth
  * Frequent full GC cycles
* Tools:

  * `process.memoryUsage()`
  * `clinic doctor`
  * Heap snapshots

### Logging & Tracing

* Correlate slow requests with:

  * Specific routes
  * User patterns
  * Payload sizes

---

## 4. Dependency & Downstream Services

**Objective:** Validate whether latency is introduced by external systems.

### Database

* Check:

  * Slow queries
  * Connection pool exhaustion
  * Missing indexes
* Tools:

  * RDS Performance Insights
  * Query execution plans

### Cache (Redis / ElastiCache)

* Validate:

  * Cache hit ratio
  * Evictions
  * Network latency

### Third-Party APIs

* Confirm:

  * Increased response times
  * Rate limiting
  * Retry storms

---

## 5. Observability Deep Dive

**Objective:** Pinpoint the exact latency contributor.

### Distributed Tracing

* Use AWS X-Ray or OpenTelemetry to:

  * Break request latency into segments
  * Identify slow spans (DB, HTTP, compute)

### Metrics to Analyze

* P50 vs P99 latency (tail latency issues)
* Saturation metrics (CPU, memory, thread pool)
* Error-latency correlation

---

## 6. Resolution Strategies

**Based on Findings**

### Infrastructure Fixes

* Scale up or out
* Switch instance types
* Tune Auto Scaling policies
* Enable connection reuse (keep-alive)

### Application Fixes

* Remove blocking code
* Introduce async workers / queues
* Add caching at appropriate layers
* Optimize serialization and payload sizes

### Database Fixes

* Add indexes
* Optimize queries
* Increase pool size (carefully)
* Introduce read replicas

---

## 7. Post-Incident Actions (Prevention)

**Objective:** Ensure the issue does not recur.

### Improvements

* Add:

  * SLOs and alerting on latency percentiles
  * Event loop lag monitoring
  * Circuit breakers and timeouts
* Introduce:

  * Load testing before releases
  * Canary deployments
  * Rate limiting

### Documentation

* Write a postmortem:

  * Root cause
  * Detection gap
  * Corrective actions

---

## 8. One-Line Interview Summary

> “I would first confirm the scope using CloudWatch and ALB metrics, then isolate whether the latency originates from infrastructure, Node.js event loop blocking, or downstream dependencies using X-Ray and runtime metrics. Once identified, I’d apply targeted fixes and follow up with observability and scaling improvements to prevent recurrence.”

---

Below is a **one-page, print-friendly Latency Debugging Checklist** tailored for a **Node.js application on AWS**. This is suitable for interviews, on-call runbooks, and real production incidents.

---

# Node.js on AWS – Latency Debugging Checklist (One Page)

## 1. Confirm & Scope the Issue (5–10 minutes)

* ☐ Latency increase confirmed (P95 / P99, not just average)
* ☐ Error rate changed? (4xx vs 5xx)
* ☐ Affects:

  * ☐ All APIs
  * ☐ Specific endpoints
  * ☐ Specific region / AZ
* ☐ Sudden or gradual increase
* ☐ Recent changes?

  * ☐ Code deployment
  * ☐ Infra / config change
  * ☐ Traffic spike

**Primary tools:** CloudWatch, ALB metrics, dashboards

---

## 2. Load Balancer & Traffic Layer

* ☐ ALB `TargetResponseTime` increased
* ☐ Healthy targets count stable
* ☐ 5xx errors from targets
* ☐ Uneven request distribution
* ☐ Sudden traffic surge (RPS)

**Conclusion check:**
If ALB latency is high → issue is **downstream**

---

## 3. Compute & Scaling (EC2 / ECS / EKS)

* ☐ CPU utilization > 70–80%
* ☐ Memory nearing limits / OOM events
* ☐ Disk I/O wait spikes
* ☐ Network latency or packet drops
* ☐ Auto Scaling:

  * ☐ Scale-out triggered?
  * ☐ Too slow or capped?
  * ☐ Correct instance type?

**Conclusion check:**
High resource usage → **capacity or scaling issue**

---

## 4. Node.js Runtime Health

* ☐ Event loop lag detected
* ☐ Synchronous/blocking code paths
* ☐ Large JSON parsing / serialization
* ☐ High GC frequency or long GC pauses
* ☐ Heap growth / memory leak symptoms

**Indicators**

* CPU high but low RPS
* Latency without DB spikes

**Tools**

* process metrics
* Event loop delay
* Heap snapshots

---

## 5. API & Code Path Analysis

* ☐ Which endpoints are slow?
* ☐ Large request/response payloads
* ☐ N+1 queries
* ☐ Excessive logging
* ☐ Missing timeouts / retries causing pile-ups

**Conclusion check:**
Only some APIs slow → **code-level issue**

---

## 6. Database Layer (RDS / DynamoDB)

* ☐ Slow queries increased
* ☐ Missing or unused indexes
* ☐ Connection pool exhaustion
* ☐ Lock contention
* ☐ Read/write latency spikes

**Tools**

* RDS Performance Insights
* Query execution plans

---

## 7. Cache Layer (Redis / ElastiCache)

* ☐ Cache hit ratio dropped
* ☐ High evictions
* ☐ Network latency to cache
* ☐ Cold cache after deployment

---

## 8. External / Third-Party Dependencies

* ☐ External API latency increased
* ☐ Timeouts or retries piling up
* ☐ Rate limiting or throttling
* ☐ DNS resolution delays

**Mitigation**

* Timeouts
* Circuit breakers
* Fallbacks

---

## 9. Tracing & Correlation (Root Cause)

* ☐ Distributed trace reviewed
* ☐ Slowest span identified
* ☐ Tail latency (P99) explained
* ☐ Error-latency correlation checked

**Tool**

* AWS X-Ray / OpenTelemetry

---

## 10. Resolution & Prevention

### Immediate Fix

* ☐ Scale up/out
* ☐ Roll back faulty release
* ☐ Disable expensive features
* ☐ Add temporary caching

### Long-Term Fix

* ☐ Optimize hot paths
* ☐ Improve Auto Scaling rules
* ☐ Add timeouts & circuit breakers
* ☐ Add alerts on P95/P99 latency
* ☐ Load test before releases

---

## One-Line Incident Summary Template

> “Latency increased due to **[root cause]**, identified via **[metric/trace]**, mitigated by **[fix]**, and prevented by **[long-term action]**.”
