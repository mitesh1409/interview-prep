# How do you handle large file uploads efficiently in Node.js?

Handling large file uploads efficiently in Node.js requires avoiding loading the entire file into memory. This is typically achieved using **streams**, **proper middleware**, and **resource limits**, along with good error handling.

### Key Principles

* Stream data in chunks instead of buffering it fully in memory
* Apply backpressure automatically using Node.js streams
* Enforce size and type limits
* Handle failures gracefully (network drops, disk errors)
* Optionally upload directly to cloud storage (S3, GCS) using streams

---

## 1. Use Streaming (Core Node.js – Most Efficient)

Node.js streams allow processing file data chunk-by-chunk, which is ideal for large uploads and prevents memory exhaustion.

### Improved Streaming Example (with error handling)

```js
const fs = require('fs');
const http = require('http');

http.createServer((req, res) => {
    const writeStream = fs.createWriteStream('uploaded_file');

    req.pipe(writeStream);

    writeStream.on('finish', () => {
        res.end('File uploaded successfully');
    });

    writeStream.on('error', (err) => {
        console.error(err);
        res.statusCode = 500;
        res.end('File upload failed');
    });

}).listen(3000);
```

**Why this works well**

* The file is never fully loaded into memory
* Node.js automatically manages backpressure
* Suitable for very large files (GBs)

---

## 2. Use Middleware (Express Applications)

In real-world Express applications, low-level stream handling is often delegated to battle-tested middleware.

### `multer` (Disk-based uploads)

```js
const express = require('express');
const multer = require('multer');

const upload = multer({
    dest: 'uploads/',
    limits: { fileSize: 10 * 1024 * 1024 } // 10 MB
});

const app = express();

app.post('/upload', upload.single('file'), (req, res) => {
    res.send('File uploaded successfully');
});

app.listen(3000);
```

### Notes

* `multer` internally uses streams
* Good for simple use cases
* For very large or streaming-only scenarios, `busboy` is more memory-efficient

---

## 3. Enforce Limits and Validation

Always protect your server:

* File size limits
* File type validation (MIME)
* Rate limiting
* Authentication for upload endpoints

Example:

```js
limits: {
  fileSize: 10 * 1024 * 1024
}
```

---

## 4. Production-Grade Enhancements (Good Interview Add-Ons)

Mentioning these shows senior-level awareness:

* **Direct streaming to cloud storage** (AWS S3, GCS, Azure Blob)
* **Resumable uploads** (tus protocol, chunked uploads)
* **Reverse proxy limits** (Nginx `client_max_body_size`)
* **Temporary disk storage cleanup**
* **Virus scanning (async post-upload)**

---

# Line-by-Line Explanation

## Approach #1 — Streaming Example

```js
const fs = require('fs');
```

* Imports Node.js File System module
* Used to write uploaded data to disk using streams

---

```js
const http = require('http');
```

* Imports the HTTP module
* Allows creation of a raw HTTP server (no Express)

---

```js
http.createServer((req, res) => {
```

* Creates an HTTP server
* `req` is a readable stream (incoming upload)
* `res` is a writable stream (server response)

---

```js
const writeStream = fs.createWriteStream('uploaded_file');
```

* Creates a writable stream pointing to a file on disk
* Data written to this stream is saved incrementally

---

```js
req.pipe(writeStream);
```

* Pipes incoming request data directly into the file stream
* Data flows **chunk by chunk**
* Backpressure is handled automatically by Node.js

---

```js
writeStream.on('finish', () => {
```

* Triggered when all data has been written successfully
* Indicates upload completion

---

```js
res.end('File uploaded successfully');
```

* Sends response to the client
* Ends the HTTP response stream

---

```js
writeStream.on('error', (err) => {
```

* Handles file system or stream errors
* Prevents silent failures

---

```js
res.statusCode = 500;
res.end('File upload failed');
```

* Returns an error response if upload fails

---

```js
}).listen(3000);
```

* Starts the server on port 3000

---

## One-Line Interview Summary (Very Effective)

> “For large file uploads in Node.js, I always use streaming to avoid loading files into memory, apply size limits, handle backpressure automatically, and in production prefer streaming directly to cloud storage.”
