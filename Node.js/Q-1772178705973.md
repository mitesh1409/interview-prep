# What are Streams in Node.js?

Streams allow reading/writing data in chunks, rather than all at once,  
which makes them efficient for handling large files or data transfers.

There are four main types of streams in Node.js:

1. Readable: for reading data (e.g., `fs.createReadStream()`).
2. Writable: for writing data (e.g., `fs.createWriteStream()`).
3. Duplex: for both reading and writing (e.g., `net.Socket`).
4. Transform: a type of duplex stream that can modify or transform data as it is read or written (e.g., `zlib.createGzip()`).

---

## Why Streams?

Before diving into streams, it's worth understanding *why* streams exist. Consider reading a 2GB file:

```javascript
// ❌ Without streams — loads entire 2GB into memory at once
const fs = require('fs');
const data = fs.readFileSync('huge-file.txt', 'utf8');
console.log(data);

// ✅ With streams — reads in small chunks, memory stays low
const stream = fs.createReadStream('huge-file.txt', 'utf8');
stream.on('data', chunk => console.log(chunk));
```

Without streams, your RAM usage spikes to the size of the file. With streams, Node.js processes it in chunks (default **64KB** each), keeping memory usage consistently low regardless of file size.

---

## 1. Readable Stream

Used for **reading data** from a source chunk by chunk.

```javascript
const fs = require('fs');

const readStream = fs.createReadStream('input.txt', { encoding: 'utf8' });

// 'data' fires every time a chunk is available
readStream.on('data', (chunk) => {
    console.log('Received chunk:', chunk);
});

// 'end' fires when there is no more data to read
readStream.on('end', () => {
    console.log('Finished reading file.');
});

// 'error' fires if something goes wrong
readStream.on('error', (err) => {
    console.error('Error reading file:', err.message);
});
```

**Common sources:** `fs.createReadStream()`, `http.IncomingMessage` (incoming HTTP request body), `process.stdin`

---

## 2. Writable Stream

Used for **writing data** to a destination chunk by chunk.

```javascript
const fs = require('fs');

const writeStream = fs.createWriteStream('output.txt');

// write() sends a chunk to the destination
writeStream.write('First chunk of data\n');
writeStream.write('Second chunk of data\n');

// end() signals no more data will be written
writeStream.end('Final chunk of data\n');

// 'finish' fires when all data has been flushed to the destination
writeStream.on('finish', () => {
    console.log('Finished writing to file.');
});

writeStream.on('error', (err) => {
    console.error('Error writing file:', err.message);
});
```

**Common destinations:** `fs.createWriteStream()`, `http.ServerResponse` (outgoing HTTP response), `process.stdout`

---

## 3. Duplex Stream

A stream that is **both Readable and Writable**, but the read and write sides are **independent** — data written in doesn't necessarily come back out. A good analogy is a telephone — you can both speak and listen, but what you say isn't what you hear back.

```javascript
const net = require('net');

// TCP server — net.Socket is a Duplex stream
const server = net.createServer((socket) => {
    // socket is Duplex — we can both read from and write to it

    // Reading side — data coming FROM the client
    socket.on('data', (data) => {
        console.log('Received from client:', data.toString());

        // Writing side — sending data TO the client
        socket.write('Server received: ' + data.toString());
    });

    socket.on('end', () => {
        console.log('Client disconnected');
    });
});

server.listen(3000, () => console.log('Server listening on port 3000'));
```

**Common examples:** `net.Socket`, `crypto.createCipheriv()`, WebSocket connections

---

## 4. Transform Stream

A **special type of Duplex stream** where the write and read sides *are* connected — data written in is **transformed** and comes back out the read side. Unlike a plain Duplex, the two sides are not independent.

A good analogy is a water filter — water goes in, filtered water comes out.

```javascript
const fs = require('fs');
const zlib = require('zlib');

// zlib.createGzip() is a Transform stream
// data written to it (readable file) comes out compressed (writable file)
const readStream = fs.createReadStream('input.txt');
const writeStream = fs.createWriteStream('input.txt.gz');
const gzip = zlib.createGzip();

readStream
    .pipe(gzip)         // Transform: compress the data
    .pipe(writeStream); // Write compressed data to file

writeStream.on('finish', () => {
    console.log('File successfully compressed!');
});
```

You can also create your own custom Transform stream:

```javascript
const { Transform } = require('stream');

// Custom transform that converts text to uppercase
const upperCaseTransform = new Transform({
    transform(chunk, encoding, callback) {
        // chunk is the incoming data
        // push() sends transformed data to the readable side
        this.push(chunk.toString().toUpperCase());
        callback(); // signal that this chunk is done processing
    }
});

process.stdin
    .pipe(upperCaseTransform)  // transform each chunk to uppercase
    .pipe(process.stdout);     // write result to terminal

// Type anything in terminal — it comes out as UPPERCASE
```

**Common examples:** `zlib.createGzip()`, `zlib.createGunzip()`, `crypto.createCipheriv()`, custom data transformation pipelines

---

## The `.pipe()` Method

You saw `.pipe()` above — it's the glue that connects streams together. It takes output from a Readable and feeds it into a Writable, automatically handling backpressure (slowing down the reader if the writer can't keep up).

```javascript
const fs = require('fs');
const zlib = require('zlib');

// Real world example — read a file, compress it, write compressed output
// without ever loading the full file into memory
fs.createReadStream('bigfile.txt')      // Readable
    .pipe(zlib.createGzip())            // Transform (compress)
    .pipe(fs.createWriteStream('bigfile.txt.gz')); // Writable
```

This is one of the most powerful patterns in Node.js — chaining streams together like Unix pipes.

---

## Backpressure — An Important Concept

Backpressure is what happens when a Writable stream can't consume data as fast as a Readable produces it. `.pipe()` handles this automatically, but it's worth knowing it exists.

```javascript
const readable = fs.createReadStream('hugefile.txt');
const writable = fs.createWriteStream('destination.txt');

// pipe() automatically pauses readable when writable is overwhelmed
// and resumes it when writable drains — this is backpressure handling
readable.pipe(writable);

// If you manage streams manually without pipe(), you have to handle this yourself:
readable.on('data', (chunk) => {
    const canContinue = writable.write(chunk);
    if (!canContinue) {
        readable.pause(); // slow down! writable buffer is full
    }
});

writable.on('drain', () => {
    readable.resume(); // writable buffer cleared, continue reading
});
```

---

## Summary

| Type | Direction | Real Example | Use Case |
|---|---|---|---|
| Readable | Read only | `fs.createReadStream()` | Reading files, HTTP request body |
| Writable | Write only | `fs.createWriteStream()` | Writing files, HTTP response |
| Duplex | Read + Write (independent) | `net.Socket` | TCP connections, WebSockets |
| Transform | Read + Write (data modified) | `zlib.createGzip()` | Compression, encryption, parsing |

The key mental shift with streams is thinking in **chunks flowing through a pipeline** rather than loading everything at once — this is what makes Node.js so efficient for I/O-heavy tasks like file processing, HTTP servers, and data pipelines.
